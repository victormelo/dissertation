%!TEX root = ../dissertation_vkslm.tex

\chapter{Experimental Evaluation}\label{ch:exp}

In order to evaluate the quality of the synthetic signatures generated by our system we follow the same protocol presented on the work of Diaz \cite{diaz2014generation}. Namely we use a state-of-the-art offline verification system and a dataset comprising both online and offline signatures in order to train the verification system to evaluate the synthetic signatures.

The goal of the experiments is to measure the quality of the synthetic signatures taking into account an offline verification system performance. The questions raised are \begin{inlinelist}
  \item is the synthetic signatures system performance similar to the real offline signatures performance?
  \item is the performance even if the enrollment protocol changes?
  \item is it feasible to increase the number of samples at the enrollment stage with synthetic signatures? 
\end{inlinelist}



\section{Off-line signature verification system}
The system used for the evaluation of the real and synthetic signatures is a Linear SVM classifier and is based on a state of the art feature extraction approach  \cite{hafemann2017learning}. The feature extraction system \footnote{https://www.etsmtl.ca/Unites-de-recherche/LIVIA/Recherche-et-innovation/Projets/Signature-Verification} uses ideas from transfer learning and multi-task learning to learn features using Convolutional Neural Networks (CNN).

\subsection {Support Vector Machine} 
SVMs represent a special class of linear classifiers. In order to classify a pattern as
belonging to one of two classes, an SVM constructs a hyperplane in the feature space,
such that it maximally separates the margin between the two classes. For this reason,
SVMs are also referred to as maximum margin classifiers.

\begin{figure*}[!htb]
\centering
 \subfloat[mono-session - random forgeries]{\includegraphics[width=2.3in]{separating-lines}} 
\hspace*{0.5in} % separation between the subfigures
\subfloat[mono-session - skilled forgeries] {\includegraphics[width=2.3in]{optimal-hyperplane}}

\caption{Boxplot comparison for running 30 times the Experiment 1 - mono-session scenario and multi-session. (a) mono-session scenario, random forgeries, (b) mono-session scenario, skilled forgeries, (c) multi-session scenario, random forgeries, (d) multi-session scenario, skilled forgeries. } \label{fig:box_exp1}
\end{figure*}

\subsection {Feature Learning} 
ideas from transfer learning and multi-task learning to learn features using Convolutional Neural Networks (CNN).

\section{Evaluation Database}


The evaluation experiments were carried out on the BiosecurID
database \cite{biosecurid}. This multimodal database was made publicly available containing signatures of 132 subjects. Signatures were
captured using a special digital inking pen on a paper placed
over a digitizing tablet. Consequently, both versions, online and
offline, of the exact same real signature were acquired at
the same time. This characteristic, therefore, makes BiosecurID the ideal benchmark for the experimental evaluation conducted in this work.

The signatures were captured in 4 sessions
distributed over 4 months. Each subject signed 4 times and
forged 3 signatures per session, thus leading to each subject having 4 genuine signatures x 4 sessions = 16 genuine samples and 3 signature forgeries x 4 sessions = 12 skilled forgeries. 

MCYT This dataset includes 75 signers collected at four different Spanish
universities. The corpus includes 15 genuine and 15 deliberately forged signatures
acquired in two sessions. All the signatures were acquired with the same inking pen
and the same paper templates. The paper templates were scanned at 600 dpi with 256
grey levels.


\section{Experimental Protocol}
In order to answer the questions stated at the beginning of this section and accomplish a fair comparison of our work and the state of the art, we follow the same experiment protocol proposed by Diaz \cite{diaz2014generation}. Two different experiments are carried out. The Experiment 1 focus on evaluating the synthetic signatures performance in comparison to real signatures and the Experiment 2 evaluates the feasibility of synthetically increasing the number of samples available in a dataset. 

For both experiments the BiosecurID dataset is split into two subsets. The first 90 users are separated as the enrollment set, used to compute the genuine and skilled impostor scores. The remaining 42 authors are considered as the test set and are used to compute the random impostor scores. The performance is evaluated in terms of equal error rate (EER), which is the point in the Detection Error Tradeoff curve (DET) where the false acceptance rate equals the false rejection rate.

\subsection{Experiment 1}

Two different protocols have been considered to
compute the 90 user enrolled models:
\begin{inlinelist}
  \item a mono-session approach, uses the four samples of the first acquisition session
  \item a multi-session approach, uses one sample of each of the 4 acquisition sessions
\end{inlinelist}.

In both cases, genuine scores are computed matching the non-enrolled genuine samples of the subject (12) to the enrolled model (90×12 = 1080 genuine scores). Random impostor scores are calculated comparing the first sample of the test subjects to the
enrolled model, leading to 90×42 = 3780 random impostor scores, and skilled impostor scores are calculated with the skilled forgeries samples of the enrolled users
(12 per subject) to the enrolled model (90×12 = 1080 skilled impostor scores).

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{multi-and-mono}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
\caption{The proposed approach diagram and an example of the synthetic signature generation.}
\label{fig:multiandmono}
\end{figure}

\subsection{Experiment 2}

This experiment is designed to assess whether synthetically increasing the enrollment dataset leads to a better recognition performance. Three different enrollment sets are considered in this experiment: 
\begin{itemize}
  \item 4 real samples belonging to the
  first acquisition session
  \item 8 real samples belonging to the first and
  the second sessions
  \item 4 real samples belonging
  to the first session plus 4 synthetic samples belonging
  to the second session.
\end{itemize}

\section{Performance Assessment}

AHSVS efficiency is quantitatively measured by two rates: False Rejection Rate (FRR) which is the percentage of genuine signatures treated as forgeries, and False
Acceptance Rate (FAR) which is the percentage of forged signatures treated as
genuine. 

A derived metric usually reported is the Average Error rate (AER) which is the average of FAR and FRR. Moreover, when experimenting an AHSVS, the trade-off between FRR and FAR must be taken into account based on the type of application and other aspects related to where the system is used. When the decision threshold of a system is set to have the FRR approximately equal to the FAR, the Equal Error Rate (EER) is calculated.

Besides quantitative results, the performance can be also compared and visualized using graphs. Receiver operating characteristics (ROC) graphs have been used increasingly in machine learning and data mining research \cite{fawcett2006introduction}. Inicialmente, a curva ROC foi desenvolvida para demonstrar as relações sinal-ruído,
interpretando o sinal como verdadeiros positivos (sensibilidade) e o ruído, os falsos positivos
(especificidade). Assim, a leitura da curva ROC é de um gráfico de sensibilidade ou taxas de
verdadeiros positivos versus taxa de falsos positivos, como apresentado na Figura 26. Os
gráficos ROC são bidimensionais, estando o eixo Y com os valores verdadeiros positivos e o
eixo X preenchido com os valores da taxa falsos positivo.

O objetivo da ferramenta curva ROC é atingir a representação perfeita do experimento,
considerando o conjunto de amostras analisado.
O objeto de estudo tem sua avaliaçãoidentificada pela localidade do ponto no gráfico. Quão mais próximo o ponto estiver do eixo Y,
melhor será o resultado. Assim, a diagonal traçada do ponto (0,0) ao ponto (1,1) serve de
direcionamento para identificar quais são os melhores resultados, estes localizados
predominantemente acima da reta [FAWCELT, 2006]. 

A métrica utilizada no estudo do ICDAR 2009 [BLANKERS et al, 2009] foi a Detection
Error Trade-off curve (DET) [MARTIN, 1997] (gráfico visualizado na Figura 28). Esta métrica
é mais conhecida como a curva DET. Esta curva corresponde a um gráfico que estabelece uma
relação entre as taxas de erro diferentemente da curva ROC. A curva é definida como a linha
contendo os pontos onde x = y, ou seja, posição onde os erros FRR e FAR possuem o mesmo
valor. Assim, o gráfico é gerado baseando-se na taxa EER. No gráfico DET o melhor resultado
está localizado o mais próximo do eixo inicial (o ponto 0 (zero)). O mais próximo apresenta,
assim, o menor erro de classificação. Esta métrica foi utilizada pelo ICDAR para definir o
algoritmo de classificação da competição com o melhor EER nos problemas de verificação de
assinaturas nos modos off-line e on-line.

\section{Statistical Evaluation}



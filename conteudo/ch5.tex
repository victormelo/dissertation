%!TEX root = ../dissertation_vkslm.tex

\chapter{Evaluation Setup}\label{ch:exp}

In order to evaluate the quality of the synthetic signatures generated by our system we follow the same protocol presented on the work of Diaz \textit{et al.} \cite{diaz2014generation}. Namely, we use a state-of-the-art offline verification system and a dataset comprising both online and offline signatures in order to train the verification system to evaluate the synthetic signatures.

The goal of the experiments is to measure the quality of the synthetic signatures taking into account an offline verification system performance. The questions raised are \begin{inlinelist}
  \item is the synthetic signatures system performance similar to the offered by real offline signatures?
  \item is it feasible to increase the number of samples at the enrollment stage with our proposed method synthetic signatures? 
\end{inlinelist}



\section{Off-line signature verification system}
The system used for the evaluation of the real and synthetic signatures is a Linear SVM classifier and with a state-of-the-art feature extraction approach  \cite{hafemann2017learning}. The feature extraction system \footnote{https://www.etsmtl.ca/Unites-de-recherche/LIVIA/Recherche-et-innovation/Projets/Signature-Verification} uses ideas from transfer learning and multi-task learning to learn features using Convolutional Neural Networks (CNN). As discussed in Chapter \ref{ch:nndl}, one of the advantages of using deep learning techniques is that some models, such as the CNN, can learn filters that can be used as feature extractors. The offline system's feature extractor takes advantage of this concept.

\subsection {Support Vector Machine} 
SVMs represent a special class of linear classifiers. In order to classify a pattern as
belonging to one of two classes, an SVM constructs a plane such that it maximally separates the margin between the two classes, as it can be seen in Figure \ref{fig:svm}. For this reason, SVMs are also referred to as maximum margin classifiers.


\begin{figure*}[!htb]
\centering
 \subfloat[The data can be separated using different. lines]{\includegraphics[width=2.3in]{separating-lines}} 
\hspace*{0.5in} % separation between the subfigures
\subfloat[The SVM strives for the optimal line, which maximally separates the margin between the two classes of data.] {\includegraphics[width=2.3in]{optimal-hyperplane}}

\caption{Illustration of several separating lines and an optimal line.} \label{fig:svm}
\end{figure*}


\section{Evaluation Database}

The evaluation experiments were carried out on the BiosecurID database \cite{biosecurid}. This multimodal database was made publicly available containing signatures of 132 subjects. Signatures were captured using a special digital inking pen on a paper placed over a digitizing tablet, exactly as shown in Figure \ref{fig:onoff}. Consequently, both versions, online and offline, of the same real signature were acquired at the same time. This characteristic, therefore, makes BiosecurID the ideal benchmark for the experimental evaluation conducted in this work.


The signatures samples were captured in 4 different sessions, distributed over four months. Each subject signed 4 times and forged 3 signatures per session, thus leading to each subject having 4 genuine signatures x 4 sessions = 16 genuine samples and 3 signature forgeries x 4 sessions = 12 skilled forgeries.

Since the offline signature verification system is trained with both positive and negative samples, to ensure an unbiased result towards the dataset selection, similar to the protocol followed by \cite{diaz2014generation} we used the MCYT database \cite{mcyt-100} as the negative dataset samples. The MCYT dataset includes 75 signers each with 15 genuine and 15 skilled forged signatures. The amount of negative samples was set to 25 random samples from the MCYT dataset.



\section{Experimental Protocol}
In order to answer the questions stated at the beginning of this section and accomplish a fair comparison of our work and the state of the art, we follow the same experiment protocol proposed by Diaz \textit{et al.} \cite{diaz2014generation}. Two different experiments are carried out. The Experiment 1 focus on evaluating the synthetic signatures performance in comparison to real signatures and the Experiment 2 evaluates the feasibility of synthetically increasing the number of samples available in a dataset. 

For both experiments, the BiosecurID dataset is split into two subsets. The first 90 users are separated as the enrollment set, used to compute the genuine and skilled impostor scores. The remaining 42 authors are considered as the test set and are used to compute the random impostor scores. The performance is evaluated regarding the equal error rate (EER), which is the point in the Detection Error Tradeoff curve (DET) where the false acceptance rate equals the false rejection rate.

\subsection{Experiment 1}

Two different protocols have been considered to
compute the 90 users enrolled models:
\begin{inlinelist}
  \item a mono-session approach, using the four samples of the first acquisition session
  \item a multi-session approach, using one sample of each of the 4 acquisition sessions
\end{inlinelist}.

In both cases, genuine scores are computed matching the non-enrolled genuine samples of the subject (12) to the enrolled model (90×12 = 1080 genuine scores). Random impostor scores are calculated comparing the first sample of the test subjects to the
enrolled model, leading to 90×42 = 3780 random impostor scores, and skilled impostor scores are calculated with the skilled forgeries samples of the enrolled users
(12 per subject) to the enrolled model (90×12 = 1080 skilled impostor scores).

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{multi-and-mono}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
\caption{The proposed approach diagram and an example of the synthetic signature generation.}
\label{fig:multiandmono}
\end{figure}

\subsection{Experiment 2}

This experiment is designed to assess whether synthetically increasing the enrollment dataset leads to a better recognition performance. Three different enrollment sets are considered in this experiment: 
\begin{itemize}
  \item 4 real samples belonging to the
  first acquisition session
  \item 8 real samples belonging to the first and
  the second sessions
  \item 4 real samples belonging
  to the first session plus 4 synthetic samples belonging
  to the second session.
\end{itemize}

\section{Performance Assessment}

AHSVS efficiency is quantitatively measured by two rates: False Rejection Rate (FRR) which is the percentage of genuine signatures treated as forgeries, and False
Acceptance Rate (FAR) which is the percentage of forged signatures treated as
genuine. 

A derived metric usually reported is the Average Error rate (AER) which is the average of FAR and FRR. Moreover, when experimenting an AHSVS, the trade-off between FRR and FAR must be taken into account based on the type of application and other aspects related to where the system is used. When the decision threshold of a system is set to have the FRR approximately equal to the FAR, the Equal Error Rate (EER) is calculated. In our experimental results, the system's EER performance is reported.

%Besides quantitative results, the performance can be also compared and visualized using graphs. Receiver operating characteristics (ROC) graphs have been used increasingly in machine learning and data mining research \cite{fawcett2006introduction}. Inicialmente, a curva ROC foi desenvolvida para demonstrar as relações sinal-ruído,
%interpretando o sinal como verdadeiros positivos (sensibilidade) e o ruído, os falsos positivos
%(especificidade). Assim, a leitura da curva ROC é de um gráfico de sensibilidade ou taxas de
%verdadeiros positivos versus taxa de falsos positivos, como apresentado na Figura 26. Os
%gráficos ROC são bidimensionais, estando o eixo Y com os valores verdadeiros positivos e o
%eixo X preenchido com os valores da taxa falsos positivo.
%
%O objetivo da ferramenta curva ROC é atingir a representação perfeita do experimento,
%considerando o conjunto de amostras analisado.
%O objeto de estudo tem sua avaliaçãoidentificada pela localidade do ponto no gráfico. Quão mais próximo o ponto estiver do eixo Y,
%melhor será o resultado. Assim, a diagonal traçada do ponto (0,0) ao ponto (1,1) serve de
%direcionamento para identificar quais são os melhores resultados, estes localizados
%predominantemente acima da reta [FAWCELT, 2006]. 
%
%A métrica utilizada no estudo do ICDAR 2009 [BLANKERS et al, 2009] foi a Detection
%Error Trade-off curve (DET) [MARTIN, 1997] (gráfico visualizado na Figura 28). Esta métrica
%é mais conhecida como a curva DET. Esta curva corresponde a um gráfico que estabelece uma
%relação entre as taxas de erro diferentemente da curva ROC. A curva é definida como a linha
%contendo os pontos onde x = y, ou seja, posição onde os erros FRR e FAR possuem o mesmo
%valor. Assim, o gráfico é gerado baseando-se na taxa EER. No gráfico DET o melhor resultado
%está localizado o mais próximo do eixo inicial (o ponto 0 (zero)). O mais próximo apresenta,
%assim, o menor erro de classificação. Esta métrica foi utilizada pelo ICDAR para definir o
%algoritmo de classificação da competição com o melhor EER nos problemas de verificação de
%assinaturas nos modos off-line e on-line.

%\section{Statistical Evaluation}


